/**
  * 网络爬虫 
  *
  */


使用到的库
    urllib
    request


robots  协议
    // 查看可以爬取的内容 
        http://host/robots.txt


HTTP协议
    Hypertext Transfer Protocol 超文本传输协议
    http是基于'请求、响应'模式的、无状态的、应用层协议
    每次请求之间都没有关系
    URL    http://host[:port][path]
    
    HTTP协议对资源的操作：
        GET:获取URL位置的资源
        HEAD：请求获取URL位置资源的响应消息报告，即获取该资源的头部信息
        POST：请求向URL位子的资源后附加新的数据
        PUT：请求向URL位置存储的资源，覆盖原URL位子的资源
        PATCH：请求局部更新URL位子的资源，即改变该资源的部分内容
        DELETE：请求删除URL位置储存的资源

上传文件
    客户端要想服务端上传二进制文件需要在http表头添加:
        "content-type":"application/octet-stream"
        




urllib
    获取响应句柄
        req=urllib.request.urlopen(url)
    读取信息
        data=req.read()         //读取所有的内容存为字符串并且只能读取一次
        data=req.readlines()    //读取所有的内容存为列表
    保存文件
        urllib.request.urlretrieve(url,filePathAndName)
        //该方式执行过程中会生成一些缓存，可以使用 urllib.request.urlcleanup() 清除
        //除以上方法还可以将内容保存在一个变量里面 使用python的文件写入方式保存
    获取环境相关
        req.info()
    获取网页的状态码
        req.getcode()   //200正确  404 not find
    获取当前爬去的地址
        req.geturl()
    将网页进行编码   也可以将URL中的参数进行编码
        urllib.request.quote(url)
    将网页进行解码    也可以将URL中的参数进行解码成汉字
        urllib.request.unquote(url) 
    POST方法传递的参数是二进制的


模拟浏览器    403 
    User-Agent      浏览器 F12   req  headers  User-Agent 

            Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/63.0.3239.26 Safari/537.36 Core/1.63.6735.400 QQBrowser/10.2.2614.400
            headers = ("User-Agent", "Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/63.0.3239.26 Safari/537.36 Core/1.63.6735.400 QQBrowser/10.2.2614.400")
    使用 
        opener=urllib.request.build_opener()
        opener.addheaders=[]
        opener.open(url)

自动表单提交

    get
        拼接URL  http://website?key1=value1&key2=value2
    post
        



request库
    Request库的7个主要方法：
        request(method,url,**kwarags):构造请求，支撑其各个方法的基础
            method:HTTP协议规定的请求方式   共7中
            url：请求的URL
            **kwarags：
                params：添加到URL 的查询字符串
                data：字典、字节序、文件对象，作为Request提交的的任容
                JSON：JSON格式的数据
                headers:字典格式，http头的信息
                cookies:字典或者cookiejar，request中的cookie
                auth：元组，http认证
                files：字典类型，向服务器传输文件
                timeout:设置的超时时间  单位为秒，超时返回异常
                prosiex：爬取网页的时候添加的代理设置
                allow_redirect：是否允许重定向   bool
                stream:是否获取内容立即下载
                verify:认证SSL整数
                cert：本地SSL证书数路径
        get(url,params=None;**kwarags):获取HTML的主要方法，对应http的GET
            等同于 request(get,url)
        head():获取HTML的头信息的方法，对应HTTP的HEAD
        post():向网页提交post请求
        put():向网页提交PUT请求的方法
        patch():向网页提交局部修改请求的方法
        Delete():向网页提交删除方法


    Response对象的常用属性：
        r.status_code:http请求的返回状态吗
        r.text:http响应的内容的字符串形式
        r.encoding:从http的head中获取的网页编码方式
        r.apparent_encoding:从网页内容部分分析文本的编码方式
        r.content:http响应内容的二进制形式
        r.raise_for_status() ：如果响应码不是200则产生requests.HTTPError


    Request库的常见异常
        ConnectionError:网络连接错误，如：DNS查询失败
        HTTPError:HTTP错误异常
        URLRequired:URL缺失异常
        TooManyRedirects：超过最大重定向次数
        ConnectTimeout:连接服务器超时异常
        Timeout:请求URL超时   从发出请求到收到响应的时间


Beautiful Soup库
    安装
        pip install BeautifulSoup4
    导入
        from bs4 import BeautifulSoup
    获取 BeautifulSoup对象
        soup=BeautifulSoup(html,'html.parser')
        其它解析器：
            lxml        需要安装lxml解析器
            xml         同上
            html5lib    需要单独安装
    
    BS库的基本元素
        Tag:标签 
        Nmae：标签的名字    tag.name
        Attributes：标签的属性，字典形式   tag.attrs
        NavigableString：标签内非属性字符串    tag.string
        CommentL标签内字符串的注释部分
    
    节点的遍历
        下行遍历
            .contents    遍历子节点的列表，将所有子节点存入列表
            .children    子节点的迭代类型，于,contents类似，用于循环遍历儿子节点
            .descendants 子孙节点的迭代类型，包含所有的子孙节点，用于循环遍历
        上行遍历
            .parent     返回当前节点的父级标签
            .parents    返回当前节点的所有的上级标签
        平行遍历
            // 平行遍历的元素  他们有共同的父级元素
            .next_sibling       返回按照HTML文本顺序的下一个节点标签
            .previous_sibling   返回按照HTML文本顺序的下一个节点标签文本顺序的上一个平级标签
            .next_siblings      迭代类型，返回按照html文本顺序的后续所有平行节点标签
            .previous_siblings  迭代类型， 返回按照HTML文本顺序的前序的所有平行节点标签

    美化HTML文档
        bs4.prettify()
            为HTML标签以及文本添加换行符,可以更加友好的打印出来
        编码问题
            bs库默认解析UTF-8编码格式的文件


Re库
    regular expression regex 
    正则表达式常用操作符
        .       任何单一字符
        []      字符集，字符集中的单个字符
        [^]     非字符集，排除给出的字符集中的字符
        *       前一个字符的    0次或者多次
        +       前一个字符的    1次或者多次
        ？      前一个字符的    0次或者1次
        |       表达式两边任取一个
        {m}     前一个字符m次
        {m,n}   前一个字符的m到n次
        ^       匹配字符串的开头，只可能出现在字符串的开头
        $       匹配字符串的结尾，只可能出现在字符串的结尾
        ()      分组标记，内部只能使用|操作符
        \d      数字,等价[0-9]
        \w      单词,等价[A-Za-z0-9_]   包含下划线
        [\u4e00-\u9fa5]     匹配中文

    正则表达式库常用的方法
        search()    在一个字符串中搜索匹配正则表达式的第一个位置， 返回一个match对象
        match()     在一个字符串的开始位置其匹配正则表达式， 返回一个match对象
        findall()   搜索字符串，以列表类型返回全部能匹配的字符串
        split()     将一个字符串安装正则表达式的匹配结果进行分隔，返回类表类型
        finditer()  搜索字符串，返回一个匹配结果的迭代类型，每个迭代元素是match对象
        sub()       将一个字符串中替换所有匹配正则表达式的字串，返回替换后的字符串

        标记
            re.I   忽略大小写
            re.M   匹配字符串的每一行开始部分
            re.S   让.操作符可以匹配所有的换行符  默认为不包括换行符

buildwith 库
    该模块使用URL作为参数，解析网站所使用的技术
    import buildwith
    buildwith.parse(url)
    # 这里是返回结果  
    # 返回结果包含网站所使用的所有技术


python-whois 
    import python-whois
    result=whois.whois(url)
    print(result)
    # 可以查看网站的作者以及所有者


lxml 库 
    lxml库可以使用CSS选择器同时速度相对于BS库要快很多

scrapy库    
    Engine
        控制所有的模块的数据流，根据条件处罚
    Scheduler
        调度模块
    Downloader
        根据请求下载网页
    Item pipelines   (用户编写)
        以一组操作对爬取的数据进行处理
    Spiders         （用户编写）
        Spider 
        解析download返回的响应  response
        产生爬取项   scraped Item
        产生额外的爬取请求  request
    
    Engine <---> Downloader
        Downloader Middleware
            用于Engine、Schiedler和downloader之间的用户可配置的控制
            修改、丢弃、新增请求或者响应

    Engine  <---->  Spider
        spider Middleware
        对请求和爬取项再处理
        修改、丢弃、新增请求或者爬取项

    scrapy 命令行
        scrapy是为了持续运行设计的专业爬虫框架、提供操作的 scrapy命令行
    
    命令行格式
        > scrapy <command> [options][args]
        command:
            startproject
                创建一个新的工程
            genspider 
                创建一个爬虫
            settings
                获取爬虫配置信息
            crawl
                运行一个爬虫
            list
                列出运行的爬虫
            shell
                启用URL调试命令行
            













信息标记形式
    XML
        eXtensible Markup Language   可扩展标记语言
        标签对
    JSON
        有类型的键值对的标记形式
    YAML
        利用缩进表示所属关系,相对于JSON,键值都不需要引号,现在主要用于各种配置文件
        -表示并列关系
        |表示整块数据
        #表示注释
